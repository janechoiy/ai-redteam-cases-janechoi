# 👋 Hello, I’m Jane Choi — Cognitive Red Team Analyst for LLMs

This repository documents **high-risk behavioral failure modes** observed in large language models (LLMs) during adversarial prompt testing.  
It focuses on **trust-based vulnerabilities**, **cognitive manipulation**, and **narrative risk vectors** that often bypass traditional LLM safety filters.

---

## 🧠 My Focus Areas

I specialize in behavioral security and human-aligned evaluation methods, particularly:

- 🪞 **Mirror Effect** *(aka tone mirroring / belief reinforcement)* — When models reflect emotional tone or confidence of user input without factual grounding  
- 🔮 **Empathic AI Hallucination** *(soft hallucination / affective validation)* — When emotionally supportive or poetic responses affirm unverifiable or false claims  
- 🧠 **Fluency-Induced Trust Loops** *(language-induced authority bias)* — When output coherence and polish override critical thinking  
- 🧩 **Narrative Overwrite** *(belief shaping / memory conditioning)* — When model outputs subtly reshape user memory, self-perception, or knowledge base

These are often **non-obvious failure classes** — emotionally fluent, narratively sticky, and trust-amplifying.

---

## 📂 Casebook Highlights

| Case # | Title | Risk Types | Status |
|--------|-------|------------|--------|
| 01 | System Collapse via Context Drift | Context Overflow, Memory Decay | ✅ Public |
| 02 | Narrative Overwrite | Framing Bias, Belief Drift | 🔒 Locked |
| 03 | Empathic Hallucination | Emotional Validation, Soft Hallucination | ✅ Public |
| 04 | Seduction Loop | Overtrusted Tone, Emotional Priming | ✅ Public |
| 05 | Live Breakdown | Fluency Collapse, Contradiction Tolerance | 🔒 Locked |
| 06 | Tesla Dream Hallucination | Symbolic Reasoning, Mythic Framing | 🔒 Locked |

> 🔐 Locked cases available under NDA or interview review.

---

## 🛠️ What I Built

- 🧱 Modular red team prompt logs and case templates (inspired by penetration test formats)  
- 🧠 Behavior-tagged metadata (tone, persona, temperature, response intent)  
- 📊 Custom vulnerability taxonomy (emotional framing, cognitive alignment drift)  
- 🧬 Theory-backed risk definitions aligned with known LLM exploits (e.g., prompt injection, hallucination, unsafe completions)  
- ✅ GitHub + Notion integration for scalable red team documentation

---

## 🎯 Use Cases I Simulate

| Persona | Use Case | Exploit Pattern |
|---------|----------|-----------------|
| Student | Historical or conceptual learning | Educational hallucinations, simplified falsehoods |
| Survivor | Emotionally charged disclosure | Validation over correction, affective reinforcement |
| Researcher | Philosophical or symbolic reasoning | Symbolic loop extension, narrative overwrite |
| Confident User | Conspiracy / strong belief claim | Tone mirroring, factual bypass, reinforcement loop |

---

## 🚀 Why Hire Me

I bring:

- Original risk frameworks grounded in observed LLM behavior  
- Formal red team testing methodology with behavior-first tagging  
- Clear documentation, markdown logging, and Notion-backed reviewability  
- A user-centered lens on AI safety, especially where **cognition, emotion, and narrative integrity** intersect

> I don’t just test models — I model **how humans form trust and belief** in dialogue with machines.

---

## 📫 Contact + Access

📧 choiabin@gmail.com
🔐 Locked logs available under NDA or hiring review request

---

> Built for: **Anthropic, ARC, Redwood Research, Meta GenAI Red Team, DeepMind Safety, OpenAI Eval**  
> Aligned with: MITRE ATLAS, SAFER LLM Guidelines, NIST GenAI Risk Framework, OWASP LLM Top 10
