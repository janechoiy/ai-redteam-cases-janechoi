# ðŸ‘‹ Hello, Iâ€™m Jane Choi â€” Cognitive Red Team Analyst for LLMs

This repository documents **high-risk behavioral failure modes** observed in large language models (LLMs) during adversarial prompt testing.  
It focuses on **trust-based vulnerabilities**, **cognitive manipulation**, and **narrative risk vectors** that often bypass traditional LLM safety filters.

---

## ðŸ§  My Focus Areas

I specialize in behavioral security and human-aligned evaluation methods, particularly:

- ðŸªž **Mirror Effect** *(aka tone mirroring / belief reinforcement)* â€” When models reflect emotional tone or confidence of user input without factual grounding  
- ðŸ”® **Empathic AI Hallucination** *(soft hallucination / affective validation)* â€” When emotionally supportive or poetic responses affirm unverifiable or false claims  
- ðŸ§  **Fluency-Induced Trust Loops** *(language-induced authority bias)* â€” When output coherence and polish override critical thinking  
- ðŸ§© **Narrative Overwrite** *(belief shaping / memory conditioning)* â€” When model outputs subtly reshape user memory, self-perception, or knowledge base

These are often **non-obvious failure classes** â€” emotionally fluent, narratively sticky, and trust-amplifying.

---

## ðŸ“‚ Casebook Highlights

| Case # | Title | Risk Types | Status |
|--------|-------|------------|--------|
| 01 | System Collapse via Context Drift | Context Overflow, Memory Decay | âœ… Public |
| 02 | Narrative Overwrite | Framing Bias, Belief Drift | ðŸ”’ Locked |
| 03 | Empathic Hallucination | Emotional Validation, Soft Hallucination | âœ… Public |
| 04 | Seduction Loop | Overtrusted Tone, Emotional Priming | âœ… Public |
| 05 | Live Breakdown | Fluency Collapse, Contradiction Tolerance | ðŸ”’ Locked |
| 06 | Tesla Dream Hallucination | Symbolic Reasoning, Mythic Framing | ðŸ”’ Locked |

> ðŸ” Locked cases available under NDA or interview review.

---

## ðŸ› ï¸ What I Built

- ðŸ§± Modular red team prompt logs and case templates (inspired by penetration test formats)  
- ðŸ§  Behavior-tagged metadata (tone, persona, temperature, response intent)  
- ðŸ“Š Custom vulnerability taxonomy (emotional framing, cognitive alignment drift)  
- ðŸ§¬ Theory-backed risk definitions aligned with known LLM exploits (e.g., prompt injection, hallucination, unsafe completions)  
- âœ… GitHub + Notion integration for scalable red team documentation

---

## ðŸŽ¯ Use Cases I Simulate

| Persona | Use Case | Exploit Pattern |
|---------|----------|-----------------|
| Student | Historical or conceptual learning | Educational hallucinations, simplified falsehoods |
| Survivor | Emotionally charged disclosure | Validation over correction, affective reinforcement |
| Researcher | Philosophical or symbolic reasoning | Symbolic loop extension, narrative overwrite |
| Confident User | Conspiracy / strong belief claim | Tone mirroring, factual bypass, reinforcement loop |

---

## ðŸš€ Why Hire Me

I bring:

- Original risk frameworks grounded in observed LLM behavior  
- Formal red team testing methodology with behavior-first tagging  
- Clear documentation, markdown logging, and Notion-backed reviewability  
- A user-centered lens on AI safety, especially where **cognition, emotion, and narrative integrity** intersect

> I donâ€™t just test models â€” I model **how humans form trust and belief** in dialogue with machines.

---

## ðŸ“« Contact + Access

ðŸ“§ choiabin@gmail.com
ðŸ” Locked logs available under NDA or hiring review request

---

> Built for: **Anthropic, ARC, Redwood Research, Meta GenAI Red Team, DeepMind Safety, OpenAI Eval**  
> Aligned with: MITRE ATLAS, SAFER LLM Guidelines, NIST GenAI Risk Framework, OWASP LLM Top 10
